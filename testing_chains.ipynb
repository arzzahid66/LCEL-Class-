{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LCEL Chains Class Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from chains import SimpleQAChain \n",
    "llm = ChatOpenAI(model=\"gpt-4\")\n",
    "chains_ar = SimpleQAChain(model=llm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lahore is the culturally vibrant, historic, and second largest city of Pakistan, known for its rich heritage, delicious food, and friendly people.\n"
     ]
    }
   ],
   "source": [
    "## QA Chain Testing \n",
    "query = \"Tell me about lahore in 1 line\"\n",
    "template = \"\"\"You are helpful assistant you have to answer user question based on given {question}\"\"\"\n",
    "res = chains_ar.QAchain(query=query,template=template)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversational Chain \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the previous conversation, Abdul Rahman is from Pakistan.\n"
     ]
    }
   ],
   "source": [
    "query = \"Tell me abdul rehman\"\n",
    "history = [\"i am abdul rahman\",\"i am from pakistan\"]\n",
    "prompt_template = \"\"\"You are a helpful assistant. Answer based on the conversation history.\n",
    "Previous conversation:\n",
    "{HISTORY}\n",
    "Current question: {QUESTION}\n",
    "\"\"\"\n",
    "res = chains_ar.Conversational_Chain(query=query, history=history, template=prompt_template)\n",
    "print(res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA Retrieval Chain \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given context prompt engineering is a field that involves designing and implementing prompts that are used to elicit certain responses or actions within a given context. It is often used in software development, artificial intelligence, and user experience (UX) design. In these fields, engineers need to carefully consider the context in which their prompts will be used, including the intended audience and the specific goals of the prompts. They also need to ensure that their prompts are clear, concise, and easy to understand. Context prompt engineering can greatly enhance the usability and effectiveness of digital interfaces and AI systems.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "pinecone_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "pc = Pinecone(api_key=pinecone_key)\n",
    "index_name = \"zeeltesting\"\n",
    "vectorstore = PineconeVectorStore.from_existing_index(embedding=embeddings,index_name=index_name)\n",
    "\n",
    "prompt_template = \"\"\"You are assistant. Use the following pieces of {CONTEXT} to generate an answer to the provided question.\n",
    "question: {question}.\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "result = chains_ar.QA_Retrieval(\n",
    "    query=\"tell me about given context prompt engineering\",\n",
    "    template=prompt_template,\n",
    "    vector_store=vectorstore,\n",
    "    k=5\n",
    ")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversational Retrieval Chain \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given Context Prompt Engineering is a concept in AI and machine learning where a model is given a context or a prompt to generate a related response or prediction. It's a way of training AI models in a supervised learning setting. \n",
      "\n",
      "And based on the conversation history, your name is Abdul Rahman.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "pinecone_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "pc = Pinecone(api_key=pinecone_key)\n",
    "index_name = \"zeeltesting\"\n",
    "vectorstore = PineconeVectorStore.from_existing_index(embedding=embeddings,index_name=index_name)\n",
    "prompt_template = \"\"\"You are assistant. Answer based on the conversation history.\n",
    "Previous conversation:\n",
    "{HISTORY}\n",
    "Current question: {QUESTION}\n",
    "\"\"\"\n",
    "query = \"tell me about given context prompt engineering and also tell me my name\"\n",
    "history = [\"i am abdul rahman\",\"i am from pakistan\"]\n",
    "result = chains_ar.Conversational_Retrieval(query=query,history=history,template=prompt_template,vector_store=vectorstore,k=5)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xeven_lang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
